{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title üîë Einmalig Global Einloggen (Fix)\n",
        "# ==============================================================================\n",
        "# F√ºhre dies EINMAL aus. Danach ist 'creds' global verf√ºgbar.\n",
        "# Alle nachfolgenden Skripte greifen darauf zu.\n",
        "# ==============================================================================\n",
        "\n",
        "# Wir nutzen die Auth-Funktion aus dem allerersten Setup-Block\n",
        "try:\n",
        "    # Speichert die Credentials global in der Variable 'creds'\n",
        "    creds = authenticate_analytics()\n",
        "    print(\"\\n‚úÖ Erfolgreich! Die Anmeldung ist nun global gespeichert.\")\n",
        "    print(\"üëâ Du kannst jetzt alle Analyse-Skripte ausf√ºhren, ohne dich neu einzuloggen.\")\n",
        "except NameError:\n",
        "    print(\"‚ùå Fehler: Bitte f√ºhre zuerst ganz oben den Block 1 (Setup & Imports) aus, damit die Funktion 'authenticate_analytics' bekannt ist.\")"
      ],
      "metadata": {
        "id": "uzVMNFagctEV",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìà 2025 Verh√§ltnis-Analyse: Organic (Basis 100) vs. LLM\n",
        "# ==============================================================================\n",
        "# LOGIK-√ÑNDERUNG:\n",
        "# Index 100 = Organischer Traffic im Januar.\n",
        "# Die LLM-Kurve zeigt das Volumen RELATIV zum organischen Startwert.\n",
        "# (Beispiel: Index 1 bei LLM bedeutet, LLM hat 1% des organischen Volumens)\n",
        "# ==============================================================================\n",
        "\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# --- KONFIGURATION ---\n",
        "START_DATE = \"2025-01-01\"\n",
        "END_DATE   = \"2025-12-31\"\n",
        "\n",
        "LLM_REGEX = r\"^.*ai|.*\\.openai.*|.*copilot.*|.*chatgpt.*|.*gemini.*|.*gpt.*|.*neeva.*|.*writesonic.*|.*nimble.*|.*outrider.*|.*perplexity.*|.*google.*bard.*|.*bard.*google.*|.*bard.*|.*edgeservices.*|.*astastic.*|.*copy\\.ai.*|.*bnngpt.*|.*gemini.*google.*$\"\n",
        "\n",
        "def get_creds_smart():\n",
        "    if 'creds' in globals() and globals()['creds'] and globals()['creds'].valid:\n",
        "        return globals()['creds']\n",
        "    try:\n",
        "        return authenticate_analytics()\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è Bitte Block 1 (Auth) zuerst ausf√ºhren.\")\n",
        "        return None\n",
        "\n",
        "def classify_traffic_source(source, medium):\n",
        "    s = str(source).lower()\n",
        "    m = str(medium).lower()\n",
        "    if re.match(LLM_REGEX, s): return \"LLM / AI\"\n",
        "    if 'organic' in m: return \"Organic Search\"\n",
        "    return \"Other\"\n",
        "\n",
        "def run_relation_analysis_2025():\n",
        "    creds = get_creds_smart()\n",
        "    if not creds: return\n",
        "\n",
        "    print(f\"\\nüöÄ Berechne Verh√§ltnis Organic vs. LLM (Basis: Organic Jan = 100)...\")\n",
        "\n",
        "    # Services & Properties\n",
        "    try:\n",
        "        admin_service = build('analyticsadmin', 'v1beta', credentials=creds)\n",
        "        data_service = build('analyticsdata', 'v1beta', credentials=creds)\n",
        "        acc_summary = admin_service.accountSummaries().list().execute()\n",
        "        all_props = [p['property'].split('/')[1] for acc in acc_summary.get('accountSummaries', []) for p in acc.get('propertySummaries', [])]\n",
        "        print(f\"üëâ Analysiere {len(all_props)} Properties...\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Fehler: {e}\")\n",
        "        return\n",
        "\n",
        "    # Datenabruf\n",
        "    daily_aggs = []\n",
        "    for p_id in tqdm(all_props, desc=\"Lade Daten\"):\n",
        "        try:\n",
        "            req = {\n",
        "                \"dateRanges\": [{\"startDate\": START_DATE, \"endDate\": END_DATE}],\n",
        "                \"dimensions\": [{\"name\": \"yearMonth\"}, {\"name\": \"sessionSource\"}, {\"name\": \"sessionMedium\"}],\n",
        "                \"metrics\": [{\"name\": \"sessions\"}]\n",
        "            }\n",
        "            res = data_service.properties().runReport(property=f\"properties/{p_id}\", body=req).execute()\n",
        "            if 'rows' not in res: continue\n",
        "\n",
        "            rows = []\n",
        "            for r in res['rows']:\n",
        "                rows.append({\n",
        "                    'Month': r['dimensionValues'][0]['value'],\n",
        "                    'Source': r['dimensionValues'][1]['value'],\n",
        "                    'Medium': r['dimensionValues'][2]['value'],\n",
        "                    'Sessions': int(r['metricValues'][0]['value'])\n",
        "                })\n",
        "            df = pd.DataFrame(rows)\n",
        "            df['Channel'] = df.apply(lambda x: classify_traffic_source(x['Source'], x['Medium']), axis=1)\n",
        "            daily_aggs.append(df.groupby(['Month', 'Channel'])['Sessions'].sum().reset_index())\n",
        "            time.sleep(0.05)\n",
        "        except: continue\n",
        "\n",
        "    if not daily_aggs:\n",
        "        print(\"‚ùå Keine Daten.\")\n",
        "        return\n",
        "\n",
        "    # Aggregation\n",
        "    full_df = pd.concat(daily_aggs)\n",
        "    total_trend = full_df.groupby(['Month', 'Channel'])['Sessions'].sum().reset_index()\n",
        "    relevant = total_trend[total_trend['Channel'].isin(['Organic Search', 'LLM / AI'])].copy()\n",
        "    pivot = relevant.pivot(index='Month', columns='Channel', values='Sessions').fillna(0)\n",
        "\n",
        "    # --- NEUE INDEX LOGIK ---\n",
        "    pivot_idx = pd.DataFrame(index=pivot.index)\n",
        "\n",
        "    try:\n",
        "        # 1. Wir holen uns den absoluten Wert von Organic im Januar (oder ersten Monat)\n",
        "        # Das ist unser universeller Teiler f√ºr ALLES.\n",
        "        base_volume = pivot['Organic Search'].iloc[0]\n",
        "\n",
        "        if base_volume == 0:\n",
        "            print(\"‚ùå Fehler: Organischer Traffic im Startmonat ist 0. Kann keine Basis bilden.\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n‚ÑπÔ∏è BASIS-WERT (Index 100): {base_volume:,.0f} Organische Sessions im Startmonat.\")\n",
        "\n",
        "        # 2. Organic Index berechnen (Startet bei 100)\n",
        "        pivot_idx['Organic Search'] = (pivot['Organic Search'] / base_volume) * 100\n",
        "\n",
        "        # 3. LLM Index berechnen (Startet bei X, relativ zu Organic)\n",
        "        # Wenn LLM 1% von Organic hat, steht hier 1.\n",
        "        pivot_idx['LLM / AI'] = (pivot['LLM / AI'] / base_volume) * 100\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Fehler bei Berechnung: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìà INDEX-TABELLE (Organic Jan = 100)\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Interpretation: Ein LLM-Wert von '2.5' bedeutet, dass LLM-Traffic\")\n",
        "    print(\"2.5% des Volumens vom organischen Traffic im Januar entspricht.\")\n",
        "    print(\"-\" * 60)\n",
        "    print(pivot_idx.round(2).to_markdown())\n",
        "\n",
        "    # --- PLOTTING ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Organic Plot\n",
        "    sns.lineplot(data=pivot_idx, x=pivot_idx.index, y='Organic Search',\n",
        "                 color='lightgray', linewidth=2, label='Organic Search (Basis)')\n",
        "\n",
        "    # LLM Plot\n",
        "    sns.lineplot(data=pivot_idx, x=pivot_idx.index, y='LLM / AI',\n",
        "                 color='#a64eff', linewidth=4, marker='o', markersize=8, label='LLM / AI (Relativ)')\n",
        "\n",
        "    # Beschriftung\n",
        "    plt.axhline(100, color='black', linestyle=':', alpha=0.3, label='Baseline (100)')\n",
        "    plt.title('Traffic-Verh√§ltnis 2025: Wie gross ist LLM im Vergleich zu Organic?', fontsize=16)\n",
        "    plt.ylabel('Index (Organic Jan = 100)', fontsize=12)\n",
        "    plt.xlabel('Monat')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.2)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_relation_analysis_2025()"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "zM1WTbXiNtgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ü§ñ LLM Solo-Analyse 2025 (Index 100 = Start)\n",
        "# ==============================================================================\n",
        "# Zeigt NUR die Entwicklung des LLM/AI Traffics.\n",
        "# Der erste Monat (Januar) wird auf 100 gesetzt.\n",
        "# Farbe: #a64eff\n",
        "# ==============================================================================\n",
        "\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# --- KONFIGURATION ---\n",
        "START_DATE = \"2025-01-01\"\n",
        "END_DATE   = \"2025-12-31\"\n",
        "\n",
        "# Dein spezifischer Regex\n",
        "LLM_REGEX = r\"^.*ai|.*\\.openai.*|.*copilot.*|.*chatgpt.*|.*gemini.*|.*gpt.*|.*neeva.*|.*writesonic.*|.*nimble.*|.*outrider.*|.*perplexity.*|.*google.*bard.*|.*bard.*google.*|.*bard.*|.*edgeservices.*|.*astastic.*|.*copy\\.ai.*|.*bnngpt.*|.*gemini.*google.*$\"\n",
        "\n",
        "def get_creds_smart():\n",
        "    if 'creds' in globals() and globals()['creds'] and globals()['creds'].valid:\n",
        "        return globals()['creds']\n",
        "    try:\n",
        "        return authenticate_analytics()\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è Bitte Block 1 (Auth) zuerst ausf√ºhren.\")\n",
        "        return None\n",
        "\n",
        "def is_llm_traffic(source):\n",
        "    \"\"\"Pr√ºft nur auf den LLM Regex.\"\"\"\n",
        "    return bool(re.match(LLM_REGEX, str(source).lower()))\n",
        "\n",
        "def run_llm_solo_analysis():\n",
        "    creds = get_creds_smart()\n",
        "    if not creds: return\n",
        "\n",
        "    print(f\"\\nüöÄ Starte isolierte LLM-Analyse 2025...\")\n",
        "\n",
        "    # Setup\n",
        "    try:\n",
        "        admin_service = build('analyticsadmin', 'v1beta', credentials=creds)\n",
        "        data_service = build('analyticsdata', 'v1beta', credentials=creds)\n",
        "\n",
        "        # Properties laden\n",
        "        acc_summary = admin_service.accountSummaries().list().execute()\n",
        "        all_props = []\n",
        "        for acc in acc_summary.get('accountSummaries', []):\n",
        "            for p in acc.get('propertySummaries', []):\n",
        "                all_props.append(p['property'].split('/')[1])\n",
        "\n",
        "        print(f\"üëâ Scanne {len(all_props)} Properties nach AI-Traffic...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Fehler: {e}\")\n",
        "        return\n",
        "\n",
        "    # Datenabruf\n",
        "    monthly_llm_sessions = []\n",
        "\n",
        "    for p_id in tqdm(all_props, desc=\"Suche AI Traffic\"):\n",
        "        try:\n",
        "            req = {\n",
        "                \"dateRanges\": [{\"startDate\": START_DATE, \"endDate\": END_DATE}],\n",
        "                \"dimensions\": [{\"name\": \"yearMonth\"}, {\"name\": \"sessionSource\"}], # Medium brauchen wir hier nicht zwingend\n",
        "                \"metrics\": [{\"name\": \"sessions\"}]\n",
        "            }\n",
        "            res = data_service.properties().runReport(property=f\"properties/{p_id}\", body=req).execute()\n",
        "\n",
        "            if 'rows' not in res: continue\n",
        "\n",
        "            for row in res['rows']:\n",
        "                source = row['dimensionValues'][1]['value']\n",
        "\n",
        "                # Filter greift direkt hier\n",
        "                if is_llm_traffic(source):\n",
        "                    month = row['dimensionValues'][0]['value']\n",
        "                    sessions = int(row['metricValues'][0]['value'])\n",
        "\n",
        "                    monthly_llm_sessions.append({'Month': month, 'Sessions': sessions})\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    if not monthly_llm_sessions:\n",
        "        print(\"‚ùå Keinen einzigen Session mit passendem LLM-Source gefunden.\")\n",
        "        return\n",
        "\n",
        "    # --- AGGREGATION ---\n",
        "    df = pd.DataFrame(monthly_llm_sessions)\n",
        "\n",
        "    # Summe aller LLM Sessions pro Monat √ºber alle Properties\n",
        "    trend = df.groupby('Month')['Sessions'].sum().reset_index().sort_values('Month')\n",
        "\n",
        "    # --- INDEX BERECHNUNG ---\n",
        "    # Basis: Erster Monat = 100\n",
        "    try:\n",
        "        base_val = trend.iloc[0]['Sessions']\n",
        "        start_month = trend.iloc[0]['Month']\n",
        "\n",
        "        if base_val == 0:\n",
        "            # Suche ersten Monat > 0\n",
        "            first_valid = trend[trend['Sessions'] > 0].iloc[0]\n",
        "            base_val = first_valid['Sessions']\n",
        "            start_month = first_valid['Month']\n",
        "            print(f\"‚ÑπÔ∏è Startmonat hatte 0 Traffic. Basis verschoben auf: {start_month}\")\n",
        "\n",
        "        trend['LLM Index'] = (trend['Sessions'] / base_val) * 100\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"ü§ñ LLM WACHSTUM 2025 (Basis: {start_month} = 100)\")\n",
        "        print(\"=\"*60)\n",
        "        print(trend[['Month', 'Sessions', 'LLM Index']].round(1).to_markdown(index=False))\n",
        "\n",
        "        # --- PLOT ---\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        sns.lineplot(data=trend, x='Month', y='LLM Index',\n",
        "                     color='#a64eff', linewidth=4, marker='o', markersize=9)\n",
        "\n",
        "        plt.axhline(100, color='gray', linestyle=':', alpha=0.5)\n",
        "        plt.title('Wachstum LLM / AI Traffic (Isoliert)', fontsize=16)\n",
        "        plt.ylabel('Index (Start = 100)', fontsize=12)\n",
        "        plt.xlabel('Monat 2025')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(True, alpha=0.2)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Fehler bei der Berechnung: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_llm_solo_analysis()"
      ],
      "metadata": {
        "id": "tVKku8G-d0WZ",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìÖ Qualit√§ts-Benchmark: LLM ist die Basis (100)\n",
        "# ==============================================================================\n",
        "# LOGIK NEU:\n",
        "# Wir berechnen den Wert f√ºr JEDEN Monat neu relativ zu LLM.\n",
        "# Formel: (Wert Organic Monat X / Wert LLM Monat X) * 100\n",
        "#\n",
        "# ERGEBNIS:\n",
        "# Die LLM-Linie wird eine flache Linie bei 100 sein.\n",
        "# Die Organic-Linie zeigt, wie viel % besser/schlechter sie im Vergleich ist.\n",
        "# ==============================================================================\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# --- KONFIGURATION ---\n",
        "START_DATE = \"2025-01-01\"\n",
        "END_DATE   = \"2025-12-31\"\n",
        "MIN_SESSIONS_MONTHLY = 20\n",
        "\n",
        "# --- GLOBALE DEFINITIONEN ---\n",
        "LLM_REGEX = r\"^.*ai|.*\\.openai.*|.*copilot.*|.*chatgpt.*|.*gemini.*|.*gpt.*|.*neeva.*|.*writesonic.*|.*nimble.*|.*outrider.*|.*perplexity.*|.*google.*bard.*|.*bard.*google.*|.*bard.*|.*edgeservices.*|.*astastic.*|.*copy\\.ai.*|.*bnngpt.*|.*gemini.*google.*$\"\n",
        "SEARCH_ENGINES = ['google', 'bing', 'yahoo', 'duckduckgo', 'ecosia', 'startpage', 'baidu', 'yandex']\n",
        "SOCIAL_SOURCES = ['facebook', 'instagram', 'linkedin', 'twitter', 't.co', 'tiktok', 'pinterest', 'reddit', 'youtube']\n",
        "\n",
        "def get_creds_smart():\n",
        "    if 'creds' in globals() and globals()['creds'] and globals()['creds'].valid:\n",
        "        return globals()['creds']\n",
        "    try:\n",
        "        return authenticate_analytics()\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è Bitte Block 1 (Auth) zuerst ausf√ºhren.\")\n",
        "        return None\n",
        "\n",
        "# --- EINHEITLICHE KLASSIFIZIERUNG F√úR BEIDE SCRIPTS ---\n",
        "def classify_universal(source, medium):\n",
        "    s = str(source).lower()\n",
        "    m = str(medium).lower()\n",
        "\n",
        "    # 1. LLM (Priorit√§t 1)\n",
        "    if re.match(LLM_REGEX, s):\n",
        "        return \"LLM / AI\"\n",
        "\n",
        "    # 2. Google Ads (Priorit√§t 2)\n",
        "    if 'google' in s and ('cpc' in m or 'ppc' in m or 'paid' in m):\n",
        "        return \"Google Ads\"\n",
        "\n",
        "    # 3. Social (Priorit√§t 3) - Wichtig: Vor Organic, damit 'insta/organic' nicht falsch z√§hlt\n",
        "    if 'social' in m or any(soc in s for soc in SOCIAL_SOURCES):\n",
        "        return \"Social\"\n",
        "\n",
        "    # 4. Organic Search (Priorit√§t 4)\n",
        "    if 'organic' in m or (m == 'referral' and any(se in s for se in SEARCH_ENGINES)):\n",
        "        return \"Organic Search\"\n",
        "\n",
        "    # 5. Direct\n",
        "    if '(direct)' in s:\n",
        "        return \"Direct\"\n",
        "\n",
        "    return \"Other\"\n",
        "\n",
        "def get_data_2025(service, property_id):\n",
        "    metrics_try = [{\"name\": \"sessions\"}, {\"name\": \"purchaseRevenue\"}, {\"name\": \"keyEvents\"}]\n",
        "    req_body = {\n",
        "        \"dateRanges\": [{\"startDate\": START_DATE, \"endDate\": END_DATE}],\n",
        "        \"dimensions\": [{\"name\": \"yearMonth\"}, {\"name\": \"sessionSource\"}, {\"name\": \"sessionMedium\"}],\n",
        "        \"metrics\": metrics_try\n",
        "    }\n",
        "    try:\n",
        "        return service.properties().runReport(property=f\"properties/{property_id}\", body=req_body).execute()\n",
        "    except: return None\n",
        "\n",
        "def run_benchmark_focused_flat():\n",
        "    creds = get_creds_smart()\n",
        "    if not creds: return\n",
        "\n",
        "    print(f\"\\nüîç Starte Benchmark (LLM = 100 Basislinie)...\")\n",
        "\n",
        "    try:\n",
        "        admin_service = build('analyticsadmin', 'v1beta', credentials=creds)\n",
        "        data_service = build('analyticsdata', 'v1beta', credentials=creds)\n",
        "        acc_summary = admin_service.accountSummaries().list().execute()\n",
        "        all_props = [{'id': p['property'].split('/')[1], 'name': p.get('displayName')}\n",
        "                     for acc in acc_summary.get('accountSummaries', [])\n",
        "                     for p in acc.get('propertySummaries', [])]\n",
        "    except: return\n",
        "\n",
        "    collected_data = []\n",
        "\n",
        "    for prop in tqdm(all_props, desc=\"Analysiere Properties\"):\n",
        "        res = get_data_2025(data_service, prop['id'])\n",
        "        if not res or 'rows' not in res: continue\n",
        "\n",
        "        data = []\n",
        "        for row in res['rows']:\n",
        "            # Metriken sicher extrahieren\n",
        "            try: rev = float(row['metricValues'][1]['value'])\n",
        "            except: rev = 0.0\n",
        "            try: conv = float(row['metricValues'][2]['value'])\n",
        "            except: conv = 0.0\n",
        "\n",
        "            data.append({\n",
        "                'MonthStr': row['dimensionValues'][0]['value'],\n",
        "                'Source': row['dimensionValues'][1]['value'],\n",
        "                'Medium': row['dimensionValues'][2]['value'],\n",
        "                'Sessions': int(row['metricValues'][0]['value']),\n",
        "                'Revenue': rev,\n",
        "                'Conversions': conv\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        if df.empty or df['Sessions'].sum() < 50: continue\n",
        "\n",
        "        df['Date'] = pd.to_datetime(df['MonthStr'], format='%Y%m')\n",
        "\n",
        "        # Klassifizierung\n",
        "        df['Channel'] = df.apply(lambda x: classify_universal(x['Source'], x['Medium']), axis=1)\n",
        "\n",
        "        # Shop-Entscheidung auf Gesamt-Daten\n",
        "        is_shop = df['Revenue'].sum() > 100\n",
        "\n",
        "        # Aggregation pro Monat\n",
        "        monthly = df.groupby(['Date', 'Channel']).agg({\n",
        "            'Sessions': 'sum', 'Revenue': 'sum', 'Conversions': 'sum'\n",
        "        }).reset_index()\n",
        "\n",
        "        # --- BENCHMARK BERECHNUNG (Monat f√ºr Monat) ---\n",
        "        # Wir iterieren durch jeden vorhandenen Monat dieser Property\n",
        "        for month_date in monthly['Date'].unique():\n",
        "            month_data = monthly[monthly['Date'] == month_date]\n",
        "\n",
        "            # 1. Hole den LLM-Wert f√ºr diesen Monat (Basis)\n",
        "            llm_row = month_data[month_data['Channel'] == 'LLM / AI']\n",
        "\n",
        "            # Wenn kein LLM Traffic in diesem Monat -> Monat √ºberspringen (kein Vergleich m√∂glich)\n",
        "            if llm_row.empty or llm_row['Sessions'].sum() < MIN_SESSIONS_MONTHLY:\n",
        "                continue\n",
        "\n",
        "            llm_sessions = llm_row['Sessions'].sum()\n",
        "            if is_shop:\n",
        "                llm_val = llm_row['Revenue'].sum() / llm_sessions\n",
        "            else:\n",
        "                llm_val = (llm_row['Conversions'].sum() / llm_sessions) * 100\n",
        "\n",
        "            if llm_val <= 0: continue\n",
        "\n",
        "            # 2. Berechne Index f√ºr Organic (und setze LLM auf 100)\n",
        "            # Wir schauen uns hier nur Organic und LLM an\n",
        "            for channel in ['LLM / AI', 'Organic Search']:\n",
        "                ch_row = month_data[month_data['Channel'] == channel]\n",
        "                if ch_row.empty or ch_row['Sessions'].sum() < MIN_SESSIONS_MONTHLY:\n",
        "                    continue\n",
        "\n",
        "                if is_shop:\n",
        "                    val = ch_row['Revenue'].sum() / ch_row['Sessions'].sum()\n",
        "                else:\n",
        "                    val = (ch_row['Conversions'].sum() / ch_row['Sessions'].sum()) * 100\n",
        "\n",
        "                # DER BENCHMARK:\n",
        "                index = (val / llm_val) * 100\n",
        "\n",
        "                collected_data.append({\n",
        "                    'Date': month_date,\n",
        "                    'Index': index,\n",
        "                    'Channel': channel\n",
        "                })\n",
        "\n",
        "        time.sleep(0.01)\n",
        "\n",
        "    # --- AUSWERTUNG ---\n",
        "    df_all = pd.DataFrame(collected_data)\n",
        "    if df_all.empty:\n",
        "        print(\"\\n‚ùå Keine vergleichbaren Daten (LLM vs Organic) gefunden.\")\n",
        "        return\n",
        "\n",
        "    # Median berechnen\n",
        "    final_trend = df_all.groupby(['Date', 'Channel'])['Index'].median().reset_index()\n",
        "\n",
        "    # Tabelle\n",
        "    pivot = final_trend.pivot(index='Date', columns='Channel', values='Index')\n",
        "    pivot.index = pivot.index.strftime('%Y-%m')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"üìä QUALIT√ÑTS-BENCHMARK (LLM ist immer 100)\")\n",
        "    print(f\"Interpretation: 150 = Organic ist 50% wertvoller als AI in diesem Monat\")\n",
        "    print(\"=\"*70)\n",
        "    print(pivot.round(1).to_markdown())\n",
        "\n",
        "    # Grafik\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # LLM (Referenz)\n",
        "    data_ai = final_trend[final_trend['Channel']=='LLM / AI']\n",
        "    sns.lineplot(data=data_ai, x='Date', y='Index',\n",
        "                 label='LLM / AI (Benchmark)', color='#a64eff', linewidth=4, linestyle='-')\n",
        "\n",
        "    # Organic\n",
        "    data_org = final_trend[final_trend['Channel']=='Organic Search']\n",
        "    if not data_org.empty:\n",
        "        sns.lineplot(data=data_org, x='Date', y='Index',\n",
        "                     label='Organic Search', color='#0F9D58', linewidth=3, marker='o')\n",
        "\n",
        "        # Label\n",
        "        last_org = data_org.iloc[-1]\n",
        "        plt.text(last_org['Date'], last_org['Index'] + 5, f\"{last_org['Index']:.0f}\",\n",
        "                 color='#0F9D58', fontweight='bold', ha='center', va='bottom')\n",
        "\n",
        "    plt.title('Ist Organic besser als AI? (Benchmark = 100)', fontsize=14)\n",
        "    plt.ylabel('Qualit√§ts-Index (LLM = 100)')\n",
        "    plt.xlabel('')\n",
        "\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
        "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
        "    plt.grid(True, alpha=0.2)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_benchmark_focused_flat()"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "td7UqE5-QNkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìä 5-Kanal Benchmark: LLM ist die Basis (100)\n",
        "# ==============================================================================\n",
        "# Vergleicht 5 Kan√§le.\n",
        "# LLM ist immer 100.\n",
        "# Organic Zahlen sind EXAKT gleich wie im ersten Script.\n",
        "# ==============================================================================\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# --- KONFIGURATION ---\n",
        "START_DATE = \"2025-01-01\"\n",
        "END_DATE   = \"2025-12-31\"\n",
        "MIN_SESSIONS_MONTHLY = 20\n",
        "\n",
        "# --- GLOBALE DEFINITIONEN (IDENTISCH) ---\n",
        "LLM_REGEX = r\"^.*ai|.*\\.openai.*|.*copilot.*|.*chatgpt.*|.*gemini.*|.*gpt.*|.*neeva.*|.*writesonic.*|.*nimble.*|.*outrider.*|.*perplexity.*|.*google.*bard.*|.*bard.*google.*|.*bard.*|.*edgeservices.*|.*astastic.*|.*copy\\.ai.*|.*bnngpt.*|.*gemini.*google.*$\"\n",
        "SEARCH_ENGINES = ['google', 'bing', 'yahoo', 'duckduckgo', 'ecosia', 'startpage', 'baidu', 'yandex']\n",
        "SOCIAL_SOURCES = ['facebook', 'instagram', 'linkedin', 'twitter', 't.co', 'tiktok', 'pinterest', 'reddit', 'youtube']\n",
        "\n",
        "def get_creds_smart():\n",
        "    if 'creds' in globals() and globals()['creds'] and globals()['creds'].valid:\n",
        "        return globals()['creds']\n",
        "    try:\n",
        "        return authenticate_analytics()\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è Bitte Block 1 (Auth) zuerst ausf√ºhren.\")\n",
        "        return None\n",
        "\n",
        "# --- EINHEITLICHE KLASSIFIZIERUNG (1:1 KOPIE VON SCRIPT 1) ---\n",
        "def classify_universal(source, medium):\n",
        "    s = str(source).lower()\n",
        "    m = str(medium).lower()\n",
        "\n",
        "    # 1. LLM\n",
        "    if re.match(LLM_REGEX, s): return \"LLM / AI\"\n",
        "    # 2. Ads\n",
        "    if 'google' in s and ('cpc' in m or 'ppc' in m or 'paid' in m): return \"Google Ads\"\n",
        "    # 3. Social\n",
        "    if 'social' in m or any(soc in s for soc in SOCIAL_SOURCES): return \"Social\"\n",
        "    # 4. Organic\n",
        "    if 'organic' in m or (m == 'referral' and any(se in s for se in SEARCH_ENGINES)): return \"Organic Search\"\n",
        "    # 5. Direct\n",
        "    if '(direct)' in s: return \"Direct\"\n",
        "\n",
        "    return \"Other\"\n",
        "\n",
        "def get_data_2025(service, property_id):\n",
        "    metrics_try = [{\"name\": \"sessions\"}, {\"name\": \"purchaseRevenue\"}, {\"name\": \"keyEvents\"}]\n",
        "    req_body = {\n",
        "        \"dateRanges\": [{\"startDate\": START_DATE, \"endDate\": END_DATE}],\n",
        "        \"dimensions\": [{\"name\": \"yearMonth\"}, {\"name\": \"sessionSource\"}, {\"name\": \"sessionMedium\"}],\n",
        "        \"metrics\": metrics_try\n",
        "    }\n",
        "    try:\n",
        "        return service.properties().runReport(property=f\"properties/{property_id}\", body=req_body).execute()\n",
        "    except: return None\n",
        "\n",
        "def run_5_channel_benchmark_flat():\n",
        "    creds = get_creds_smart()\n",
        "    if not creds: return\n",
        "\n",
        "    print(f\"\\nüîç Starte 5-Kanal Benchmark (Synchronisiert)...\")\n",
        "\n",
        "    try:\n",
        "        admin_service = build('analyticsadmin', 'v1beta', credentials=creds)\n",
        "        data_service = build('analyticsdata', 'v1beta', credentials=creds)\n",
        "        acc_summary = admin_service.accountSummaries().list().execute()\n",
        "        all_props = [{'id': p['property'].split('/')[1], 'name': p.get('displayName')}\n",
        "                     for acc in acc_summary.get('accountSummaries', [])\n",
        "                     for p in acc.get('propertySummaries', [])]\n",
        "    except: return\n",
        "\n",
        "    collected_data = []\n",
        "\n",
        "    for prop in tqdm(all_props, desc=\"Verarbeite Properties\"):\n",
        "        res = get_data_2025(data_service, prop['id'])\n",
        "        if not res or 'rows' not in res: continue\n",
        "\n",
        "        data = []\n",
        "        for row in res['rows']:\n",
        "            try: rev = float(row['metricValues'][1]['value'])\n",
        "            except: rev = 0.0\n",
        "            try: conv = float(row['metricValues'][2]['value'])\n",
        "            except: conv = 0.0\n",
        "\n",
        "            data.append({\n",
        "                'MonthStr': row['dimensionValues'][0]['value'],\n",
        "                'Source': row['dimensionValues'][1]['value'],\n",
        "                'Medium': row['dimensionValues'][2]['value'],\n",
        "                'Sessions': int(row['metricValues'][0]['value']),\n",
        "                'Revenue': rev,\n",
        "                'Conversions': conv\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        if df.empty or df['Sessions'].sum() < 50: continue\n",
        "\n",
        "        df['Date'] = pd.to_datetime(df['MonthStr'], format='%Y%m')\n",
        "\n",
        "        # Klassifizierung\n",
        "        df['Channel'] = df.apply(lambda x: classify_universal(x['Source'], x['Medium']), axis=1)\n",
        "\n",
        "        # Shop Entscheidung (global)\n",
        "        is_shop = df['Revenue'].sum() > 100\n",
        "\n",
        "        # Aggregation\n",
        "        monthly = df.groupby(['Date', 'Channel']).agg({\n",
        "            'Sessions': 'sum', 'Revenue': 'sum', 'Conversions': 'sum'\n",
        "        }).reset_index()\n",
        "\n",
        "        # --- BENCHMARK PRO MONAT ---\n",
        "        relevant_channels = ['LLM / AI', 'Google Ads', 'Social', 'Organic Search', 'Direct']\n",
        "\n",
        "        for month_date in monthly['Date'].unique():\n",
        "            month_data = monthly[monthly['Date'] == month_date]\n",
        "\n",
        "            # 1. LLM Basis f√ºr diesen Monat holen\n",
        "            llm_row = month_data[month_data['Channel'] == 'LLM / AI']\n",
        "\n",
        "            if llm_row.empty or llm_row['Sessions'].sum() < MIN_SESSIONS_MONTHLY:\n",
        "                continue\n",
        "\n",
        "            llm_sessions = llm_row['Sessions'].sum()\n",
        "            if is_shop:\n",
        "                llm_val = llm_row['Revenue'].sum() / llm_sessions\n",
        "            else:\n",
        "                llm_val = (llm_row['Conversions'].sum() / llm_sessions) * 100\n",
        "\n",
        "            if llm_val <= 0: continue\n",
        "\n",
        "            # 2. Andere Kan√§le dagegen messen\n",
        "            for channel in relevant_channels:\n",
        "                ch_row = month_data[month_data['Channel'] == channel]\n",
        "                if ch_row.empty or ch_row['Sessions'].sum() < MIN_SESSIONS_MONTHLY:\n",
        "                    continue\n",
        "\n",
        "                if is_shop:\n",
        "                    val = ch_row['Revenue'].sum() / ch_row['Sessions'].sum()\n",
        "                else:\n",
        "                    val = (ch_row['Conversions'].sum() / ch_row['Sessions'].sum()) * 100\n",
        "\n",
        "                # INDEX\n",
        "                index = (val / llm_val) * 100\n",
        "\n",
        "                collected_data.append({\n",
        "                    'Date': month_date,\n",
        "                    'Index': index,\n",
        "                    'Channel': channel\n",
        "                })\n",
        "\n",
        "        time.sleep(0.01)\n",
        "\n",
        "    # --- AUSWERTUNG ---\n",
        "    if not collected_data:\n",
        "        print(\"\\n‚ùå Keine Daten.\")\n",
        "        return\n",
        "\n",
        "    df_all = pd.DataFrame(collected_data)\n",
        "\n",
        "    # Median Aggregation\n",
        "    final_trend = df_all.groupby(['Date', 'Channel'])['Index'].median().reset_index()\n",
        "\n",
        "    # Tabelle\n",
        "    pivot = final_trend.pivot(index='Date', columns='Channel', values='Index')\n",
        "    pivot.index = pivot.index.strftime('%Y-%m')\n",
        "\n",
        "    desired_order = ['LLM / AI', 'Organic Search', 'Google Ads', 'Social', 'Direct']\n",
        "    cols = [c for c in desired_order if c in pivot.columns]\n",
        "    pivot = pivot[cols]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"üìä 5-KANAL BENCHMARK (LLM = 100 Basis)\")\n",
        "    print(\"=\"*70)\n",
        "    print(pivot.round(1).to_markdown())\n",
        "\n",
        "    # --- PLOTTING ---\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    colors = {\n",
        "        'LLM / AI': '#a64eff',       # Violett\n",
        "        'Google Ads': '#DB4437',     # Rot\n",
        "        'Social': '#4285F4',         # Blau\n",
        "        'Organic Search': '#0F9D58', # Gr√ºn\n",
        "        'Direct': '#757575'          # Grau\n",
        "    }\n",
        "\n",
        "    for channel in cols:\n",
        "        subset = final_trend[final_trend['Channel'] == channel]\n",
        "\n",
        "        lw = 4 if channel == 'LLM / AI' else (3 if channel == 'Organic Search' else 1.5)\n",
        "        style = '-' # Alle solid, LLM dicker\n",
        "        alpha = 1.0 if channel in ['LLM / AI', 'Organic Search'] else 0.7\n",
        "\n",
        "        sns.lineplot(data=subset, x='Date', y='Index',\n",
        "                     label=channel, color=colors.get(channel, 'black'),\n",
        "                     linewidth=lw, linestyle=style, marker='o', alpha=alpha)\n",
        "\n",
        "        if not subset.empty:\n",
        "            last = subset.iloc[-1]\n",
        "            plt.text(last['Date'], last['Index'], f\" {last['Index']:.0f}\",\n",
        "                     color=colors.get(channel, 'black'), fontweight='bold', va='center')\n",
        "\n",
        "    plt.axhline(100, color='black', linestyle=':', alpha=0.4)\n",
        "    plt.title('Qualit√§ts-Vergleich: Wer schl√§gt den AI-Traffic?', fontsize=16)\n",
        "    plt.ylabel('Qualit√§ts-Index (LLM = 100)', fontsize=12)\n",
        "    plt.xlabel('')\n",
        "\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
        "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
        "    plt.grid(True, alpha=0.2)\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_5_channel_benchmark_flat()"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "LTW6nR4ZRnd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üí∞ Conversion Rate Analyse 2025: Organic vs. LLM\n",
        "# ==============================================================================\n",
        "# Vergleicht die Qualit√§t des Traffics (KeyEvents / Sessions * 100)\n",
        "# Farbe LLM: #a64eff\n",
        "# Zeitraum: Jan - Dez 2025\n",
        "# ==============================================================================\n",
        "\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "from tqdm import tqdm\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# --- KONFIGURATION ---\n",
        "START_DATE = \"2025-01-01\"\n",
        "END_DATE   = \"2025-12-31\"\n",
        "\n",
        "# Dein spezifischer Regex\n",
        "LLM_REGEX = r\"^.*ai|.*\\.openai.*|.*copilot.*|.*chatgpt.*|.*gemini.*|.*gpt.*|.*neeva.*|.*writesonic.*|.*nimble.*|.*outrider.*|.*perplexity.*|.*google.*bard.*|.*bard.*google.*|.*bard.*|.*edgeservices.*|.*astastic.*|.*copy\\.ai.*|.*bnngpt.*|.*gemini.*google.*$\"\n",
        "\n",
        "def get_creds_smart():\n",
        "    if 'creds' in globals() and globals()['creds'] and globals()['creds'].valid:\n",
        "        return globals()['creds']\n",
        "    try:\n",
        "        return authenticate_analytics()\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è Bitte Block 1 (Auth) zuerst ausf√ºhren.\")\n",
        "        return None\n",
        "\n",
        "def classify_traffic_source(source, medium):\n",
        "    s = str(source).lower()\n",
        "    m = str(medium).lower()\n",
        "\n",
        "    # 1. Regex Check (Priorit√§t)\n",
        "    if re.match(LLM_REGEX, s):\n",
        "        return \"LLM / AI\"\n",
        "\n",
        "    # 2. Organic Search\n",
        "    if 'organic' in m:\n",
        "        return \"Organic Search\"\n",
        "\n",
        "    return \"Other\"\n",
        "\n",
        "def get_conversion_data(service, property_id):\n",
        "    \"\"\"Holt Sessions UND KeyEvents (mit Fallback auf Conversions).\"\"\"\n",
        "    # Wir brauchen beides: Sessions (Nenner) und KeyEvents (Z√§hler)\n",
        "    metrics_try = [{\"name\": \"sessions\"}, {\"name\": \"keyEvents\"}]\n",
        "\n",
        "    req = {\n",
        "        \"dateRanges\": [{\"startDate\": START_DATE, \"endDate\": END_DATE}],\n",
        "        \"dimensions\": [{\"name\": \"yearMonth\"}, {\"name\": \"sessionSource\"}, {\"name\": \"sessionMedium\"}],\n",
        "        \"metrics\": metrics_try\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        return service.properties().runReport(property=f\"properties/{property_id}\", body=req).execute()\n",
        "    except Exception as e:\n",
        "        # Fallback f√ºr alte Properties (legacy 'conversions')\n",
        "        if \"keyEvents\" in str(e) or \"400\" in str(e):\n",
        "            req['metrics'] = [{\"name\": \"sessions\"}, {\"name\": \"conversions\"}]\n",
        "            try:\n",
        "                return service.properties().runReport(property=f\"properties/{property_id}\", body=req).execute()\n",
        "            except:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "def run_cvr_analysis_2025():\n",
        "    creds = get_creds_smart()\n",
        "    if not creds: return\n",
        "\n",
        "    print(f\"\\nüöÄ Berechne Conversion Rate Vergleich (2025)...\")\n",
        "\n",
        "    # Services\n",
        "    try:\n",
        "        admin_service = build('analyticsadmin', 'v1beta', credentials=creds)\n",
        "        data_service = build('analyticsdata', 'v1beta', credentials=creds)\n",
        "\n",
        "        acc_summary = admin_service.accountSummaries().list().execute()\n",
        "        all_props = []\n",
        "        for acc in acc_summary.get('accountSummaries', []):\n",
        "            for p in acc.get('propertySummaries', []):\n",
        "                all_props.append(p['property'].split('/')[1])\n",
        "\n",
        "        print(f\"üëâ Analysiere Qualit√§t von {len(all_props)} Properties...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Fehler: {e}\")\n",
        "        return\n",
        "\n",
        "    # Daten sammeln\n",
        "    raw_data = []\n",
        "\n",
        "    for p_id in tqdm(all_props, desc=\"Lade Daten\"):\n",
        "        res = get_conversion_data(data_service, p_id)\n",
        "\n",
        "        if not res or 'rows' not in res: continue\n",
        "\n",
        "        for r in res['rows']:\n",
        "            sessions = int(r['metricValues'][0]['value'])\n",
        "            conversions = float(r['metricValues'][1]['value'])\n",
        "\n",
        "            # Nur Daten speichern, wenn Traffic da ist\n",
        "            if sessions > 0:\n",
        "                raw_data.append({\n",
        "                    'Month': r['dimensionValues'][0]['value'],\n",
        "                    'Source': r['dimensionValues'][1]['value'],\n",
        "                    'Medium': r['dimensionValues'][2]['value'],\n",
        "                    'Sessions': sessions,\n",
        "                    'Conversions': conversions\n",
        "                })\n",
        "\n",
        "        time.sleep(0.05)\n",
        "\n",
        "    if not raw_data:\n",
        "        print(\"‚ùå Keine Daten verf√ºgbar.\")\n",
        "        return\n",
        "\n",
        "    # --- AGGREGATION ---\n",
        "    df = pd.DataFrame(raw_data)\n",
        "\n",
        "    # Klassifizieren\n",
        "    df['Channel'] = df.apply(lambda x: classify_traffic_source(x['Source'], x['Medium']), axis=1)\n",
        "\n",
        "    # Wir filtern nur Organic und LLM\n",
        "    df = df[df['Channel'].isin(['Organic Search', 'LLM / AI'])]\n",
        "\n",
        "    # Gruppieren √ºber ALLE Properties (Portfolio View)\n",
        "    # Summe Conversions / Summe Sessions = Wahre Portfolio CVR\n",
        "    grouped = df.groupby(['Month', 'Channel']).agg({\n",
        "        'Sessions': 'sum',\n",
        "        'Conversions': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # CVR Berechnen\n",
        "    grouped['CVR %'] = (grouped['Conversions'] / grouped['Sessions']) * 100\n",
        "\n",
        "    # Pivot f√ºr Tabelle\n",
        "    pivot = grouped.pivot(index='Month', columns='Channel', values='CVR %').fillna(0)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üí∞ CONVERSION RATE (%) IM VERGLEICH\")\n",
        "    print(\"=\"*60)\n",
        "    print(pivot.round(2).to_markdown())\n",
        "\n",
        "    # --- PLOTTING ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Organic Linie\n",
        "    sns.lineplot(data=grouped[grouped['Channel']=='Organic Search'], x='Month', y='CVR %',\n",
        "                 color='gray', linewidth=2, label='Organic Search (CVR)')\n",
        "\n",
        "    # LLM Linie\n",
        "    sns.lineplot(data=grouped[grouped['Channel']=='LLM / AI'], x='Month', y='CVR %',\n",
        "                 color='#a64eff', linewidth=4, marker='o', markersize=8, label='LLM / AI (CVR)')\n",
        "\n",
        "    plt.title('Qualit√§ts-Vergleich 2025: Conversion Rate (Key Events / Sessions)', fontsize=15)\n",
        "    plt.ylabel('Conversion Rate (%)', fontsize=12)\n",
        "    plt.xlabel('Monat')\n",
        "\n",
        "    # Y-Achse als Prozent formatieren\n",
        "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.2)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_cvr_analysis_2025()"
      ],
      "metadata": {
        "id": "puM1ZO2re4SK",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üí∞ CVR Analyse 2025: Median pro Property (Fairer Vergleich)\n",
        "# ==============================================================================\n",
        "# Berechnet CVR pro Property und bildet dann den Median.\n",
        "# Verhindert, dass grosse Accounts den Durchschnitt verzerren.\n",
        "# Farbe LLM: #a64eff\n",
        "# ==============================================================================\n",
        "\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "from tqdm import tqdm\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# --- KONFIGURATION ---\n",
        "START_DATE = \"2025-01-01\"\n",
        "END_DATE   = \"2025-12-31\"\n",
        "MIN_SESSIONS = 50 # Mindest-Traffic pro Monat/Property, um 0% CVRs durch Rauschen zu filtern\n",
        "\n",
        "LLM_REGEX = r\"^.*ai|.*\\.openai.*|.*copilot.*|.*chatgpt.*|.*gemini.*|.*gpt.*|.*neeva.*|.*writesonic.*|.*nimble.*|.*outrider.*|.*perplexity.*|.*google.*bard.*|.*bard.*google.*|.*bard.*|.*edgeservices.*|.*astastic.*|.*copy\\.ai.*|.*bnngpt.*|.*gemini.*google.*$\"\n",
        "\n",
        "def get_creds_smart():\n",
        "    if 'creds' in globals() and globals()['creds'] and globals()['creds'].valid:\n",
        "        return globals()['creds']\n",
        "    try:\n",
        "        return authenticate_analytics()\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è Bitte Block 1 (Auth) zuerst ausf√ºhren.\")\n",
        "        return None\n",
        "\n",
        "def classify_traffic_source(source, medium):\n",
        "    s = str(source).lower()\n",
        "    m = str(medium).lower()\n",
        "    if re.match(LLM_REGEX, s): return \"LLM / AI\"\n",
        "    if 'organic' in m: return \"Organic Search\"\n",
        "    return \"Other\"\n",
        "\n",
        "def get_conversion_data_safe(service, property_id):\n",
        "    \"\"\"Holt Daten mit Fallback.\"\"\"\n",
        "    metrics_try = [{\"name\": \"sessions\"}, {\"name\": \"keyEvents\"}]\n",
        "    req = {\n",
        "        \"dateRanges\": [{\"startDate\": START_DATE, \"endDate\": END_DATE}],\n",
        "        \"dimensions\": [{\"name\": \"yearMonth\"}, {\"name\": \"sessionSource\"}, {\"name\": \"sessionMedium\"}],\n",
        "        \"metrics\": metrics_try\n",
        "    }\n",
        "    try:\n",
        "        return service.properties().runReport(property=f\"properties/{property_id}\", body=req).execute()\n",
        "    except Exception as e:\n",
        "        if \"keyEvents\" in str(e) or \"400\" in str(e):\n",
        "            req['metrics'] = [{\"name\": \"sessions\"}, {\"name\": \"conversions\"}]\n",
        "            try:\n",
        "                return service.properties().runReport(property=f\"properties/{property_id}\", body=req).execute()\n",
        "            except: return None\n",
        "        return None\n",
        "\n",
        "def run_median_cvr_analysis():\n",
        "    creds = get_creds_smart()\n",
        "    if not creds: return\n",
        "\n",
        "    print(f\"\\nüöÄ Berechne MEDIAN Conversion Rates (Property-Level)...\")\n",
        "\n",
        "    # Setup\n",
        "    try:\n",
        "        admin_service = build('analyticsadmin', 'v1beta', credentials=creds)\n",
        "        data_service = build('analyticsdata', 'v1beta', credentials=creds)\n",
        "        acc_summary = admin_service.accountSummaries().list().execute()\n",
        "        all_props = [p['property'].split('/')[1] for acc in acc_summary.get('accountSummaries', []) for p in acc.get('propertySummaries', [])]\n",
        "        print(f\"üëâ Analysiere {len(all_props)} Properties...\")\n",
        "    except: return\n",
        "\n",
        "    # Daten sammeln\n",
        "    prop_cvrs = []\n",
        "\n",
        "    for p_id in tqdm(all_props, desc=\"Lade Daten\"):\n",
        "        res = get_conversion_data_safe(data_service, p_id)\n",
        "        if not res or 'rows' not in res: continue\n",
        "\n",
        "        # Lokaler DataFrame f√ºr diese Property\n",
        "        rows = []\n",
        "        for r in res['rows']:\n",
        "            rows.append({\n",
        "                'Month': r['dimensionValues'][0]['value'],\n",
        "                'Source': r['dimensionValues'][1]['value'],\n",
        "                'Medium': r['dimensionValues'][2]['value'],\n",
        "                'Sessions': int(r['metricValues'][0]['value']),\n",
        "                'Conversions': float(r['metricValues'][1]['value'])\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(rows)\n",
        "        if df.empty: continue\n",
        "\n",
        "        # Klassifizieren\n",
        "        df['Channel'] = df.apply(lambda x: classify_traffic_source(x['Source'], x['Medium']), axis=1)\n",
        "        df = df[df['Channel'].isin(['Organic Search', 'LLM / AI'])]\n",
        "\n",
        "        # Aggregation PRO PROPERTY\n",
        "        grouped = df.groupby(['Month', 'Channel']).agg({'Sessions': 'sum', 'Conversions': 'sum'}).reset_index()\n",
        "\n",
        "        # Filter: Nur Monate mit signifikantem Traffic ber√ºcksichtigen (kein Rauschen)\n",
        "        grouped = grouped[grouped['Sessions'] >= MIN_SESSIONS]\n",
        "\n",
        "        # CVR berechnen\n",
        "        grouped['CVR'] = (grouped['Conversions'] / grouped['Sessions']) * 100\n",
        "\n",
        "        for _, row in grouped.iterrows():\n",
        "            prop_cvrs.append({\n",
        "                'Month': row['Month'],\n",
        "                'Channel': row['Channel'],\n",
        "                'CVR': row['CVR'],\n",
        "                'PropertyID': p_id\n",
        "            })\n",
        "\n",
        "        time.sleep(0.05)\n",
        "\n",
        "    if not prop_cvrs:\n",
        "        print(\"‚ùå Keine Daten gefunden.\")\n",
        "        return\n",
        "\n",
        "    # --- MEDIAN BERECHNUNG ---\n",
        "    full_df = pd.DataFrame(prop_cvrs)\n",
        "\n",
        "    # Wir gruppieren nach Monat und Kanal und nehmen den MEDIAN der CVRs aller Properties\n",
        "    median_trend = full_df.groupby(['Month', 'Channel'])['CVR'].median().reset_index()\n",
        "\n",
        "    # Pivot f√ºr Tabelle\n",
        "    pivot = median_trend.pivot(index='Month', columns='Channel', values='CVR').fillna(0)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üí∞ MEDIAN CVR (%) - TYPISCHE PROPERTY PERFORMANCE\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Interpretation: '1.5%' bedeutet, dass 50% deiner Webseiten\")\n",
        "    print(\"eine bessere und 50% eine schlechtere Conversion Rate haben.\")\n",
        "    print(\"-\" * 60)\n",
        "    print(pivot.round(2).to_markdown())\n",
        "\n",
        "    # --- PLOTTING ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    sns.lineplot(data=median_trend[median_trend['Channel']=='Organic Search'], x='Month', y='CVR',\n",
        "                 color='gray', linewidth=2, label='Organic Search (Median)')\n",
        "\n",
        "    sns.lineplot(data=median_trend[median_trend['Channel']=='LLM / AI'], x='Month', y='CVR',\n",
        "                 color='#a64eff', linewidth=4, marker='o', markersize=8, label='LLM / AI (Median)')\n",
        "\n",
        "    plt.title('Performance Vergleich: Median Conversion Rate (Portfolio)', fontsize=15)\n",
        "    plt.ylabel('Median Conversion Rate (%)', fontsize=12)\n",
        "    plt.xlabel('Monat')\n",
        "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.2)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_median_cvr_analysis()"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "DI9S9xW-PL-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üíé RPS Analyse (Streng: Jeden Monat 1+ Purchase, Weighted Avg)\n",
        "# ==============================================================================\n",
        "# Filter: Nur Shops, die in JEDEM der 12 Monate mind. 1 Purchase hatten.\n",
        "# Aggregation: Gewichteter Durchschnitt (Summe Revenue / Summe Sessions).\n",
        "# Farbe LLM: #a64eff\n",
        "# ==============================================================================\n",
        "\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from tqdm import tqdm\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# --- KONFIGURATION ---\n",
        "START_DATE = \"2025-01-01\"\n",
        "END_DATE   = \"2025-12-31\"\n",
        "MIN_SESSIONS_MONTHLY = 10\n",
        "\n",
        "LLM_REGEX = r\"^.*ai|.*\\.openai.*|.*copilot.*|.*chatgpt.*|.*gemini.*|.*gpt.*|.*neeva.*|.*writesonic.*|.*nimble.*|.*outrider.*|.*perplexity.*|.*google.*bard.*|.*bard.*google.*|.*bard.*|.*edgeservices.*|.*astastic.*|.*copy\\.ai.*|.*bnngpt.*|.*gemini.*google.*$\"\n",
        "\n",
        "def get_creds_smart():\n",
        "    if 'creds' in globals() and globals()['creds'] and globals()['creds'].valid:\n",
        "        return globals()['creds']\n",
        "    try:\n",
        "        return authenticate_analytics()\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è Bitte Block 1 (Auth) zuerst ausf√ºhren.\")\n",
        "        return None\n",
        "\n",
        "def classify_traffic_source(source, medium):\n",
        "    s = str(source).lower()\n",
        "    m = str(medium).lower()\n",
        "    if re.match(LLM_REGEX, s): return \"LLM / AI\"\n",
        "    if 'organic' in m: return \"Organic Search\"\n",
        "    return \"Other\"\n",
        "\n",
        "def get_ecommerce_data(service, property_id):\n",
        "    req = {\n",
        "        \"dateRanges\": [{\"startDate\": START_DATE, \"endDate\": END_DATE}],\n",
        "        \"dimensions\": [{\"name\": \"yearMonth\"}, {\"name\": \"sessionSource\"}, {\"name\": \"sessionMedium\"}],\n",
        "        \"metrics\": [{\"name\": \"sessions\"}, {\"name\": \"ecommercePurchases\"}, {\"name\": \"purchaseRevenue\"}]\n",
        "    }\n",
        "    try:\n",
        "        return service.properties().runReport(property=f\"properties/{property_id}\", body=req).execute()\n",
        "    except: return None\n",
        "\n",
        "def check_monthly_activity(df):\n",
        "    \"\"\"Pr√ºft, ob in JEDEM Monat Conversions stattfanden.\"\"\"\n",
        "    # Wir gruppieren alle Monate, in denen Purchases > 0 waren\n",
        "    active_months = df[df['Purchases'] > 0]['Month'].unique()\n",
        "    # Da wir 2025 analysieren, erwarten wir idealerweise 12 Monate (oder so viele wie API liefert)\n",
        "    # Wir setzen die H√ºrde auf >= 10 aktive Monate, um kleine Datenl√ºcken zu verzeihen,\n",
        "    # oder strikt auf 12. Hier: Strikt auf die Anzahl der Monate, die im Datensatz sind.\n",
        "    return len(active_months) >= 12\n",
        "\n",
        "def run_strict_rps_analysis():\n",
        "    creds = get_creds_smart()\n",
        "    if not creds: return\n",
        "\n",
        "    print(f\"\\nüöÄ Berechne Weighted RPS (Nur Shops mit Sales in JEDEM Monat)...\")\n",
        "\n",
        "    # Init\n",
        "    try:\n",
        "        admin_service = build('analyticsadmin', 'v1beta', credentials=creds)\n",
        "        data_service = build('analyticsdata', 'v1beta', credentials=creds)\n",
        "        acc_summary = admin_service.accountSummaries().list().execute()\n",
        "        all_props = [{'id': p['property'].split('/')[1], 'name': p.get('displayName')}\n",
        "                     for acc in acc_summary.get('accountSummaries', [])\n",
        "                     for p in acc.get('propertySummaries', [])]\n",
        "    except: return\n",
        "\n",
        "    valid_shops_data = []\n",
        "    qualified_props_count = 0\n",
        "\n",
        "    for prop in tqdm(all_props, desc=\"Filtere Shops\"):\n",
        "        res = get_ecommerce_data(data_service, prop['id'])\n",
        "        if not res or 'rows' not in res: continue\n",
        "\n",
        "        # Raw Data Parsing\n",
        "        rows = []\n",
        "        for r in res['rows']:\n",
        "            rows.append({\n",
        "                'Month': r['dimensionValues'][0]['value'],\n",
        "                'Source': r['dimensionValues'][1]['value'],\n",
        "                'Medium': r['dimensionValues'][2]['value'],\n",
        "                'Sessions': int(r['metricValues'][0]['value']),\n",
        "                'Purchases': int(r['metricValues'][1]['value']),\n",
        "                'Revenue': float(r['metricValues'][2]['value'])\n",
        "            })\n",
        "\n",
        "        df_raw = pd.DataFrame(rows)\n",
        "        if df_raw.empty: continue\n",
        "\n",
        "        # --- FILTER: JEDEN MONAT SALES? ---\n",
        "        # Wir summieren erst alle K√§ufe pro Monat (egal welcher Kanal)\n",
        "        monthly_total = df_raw.groupby('Month')['Purchases'].sum().reset_index()\n",
        "\n",
        "        # Pr√ºfung: Gibt es 12 Monate mit Purchases > 0?\n",
        "        # (Wir nehmen an, das Jahr hat 12 Monate im Report)\n",
        "        months_with_sales = monthly_total[monthly_total['Purchases'] > 0]\n",
        "\n",
        "        if len(months_with_sales) < 12:\n",
        "            continue # Property rauswerfen\n",
        "\n",
        "        qualified_props_count += 1\n",
        "\n",
        "        # Klassifizierung\n",
        "        df_raw['Channel'] = df_raw.apply(lambda x: classify_traffic_source(x['Source'], x['Medium']), axis=1)\n",
        "        df_relevant = df_raw[df_raw['Channel'].isin(['Organic Search', 'LLM / AI'])]\n",
        "\n",
        "        if df_relevant.empty: continue\n",
        "\n",
        "        valid_shops_data.append(df_relevant)\n",
        "\n",
        "    if not valid_shops_data:\n",
        "        print(\"‚ùå Keine Property gefunden, die in JEDEM Monat Sales hatte.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n‚úÖ {qualified_props_count} 'Dauerl√§ufer-Shops' identifiziert und aggregiert.\")\n",
        "\n",
        "    # --- AGGREGATION (WEIGHTED AVERAGE) ---\n",
        "    full_df = pd.concat(valid_shops_data)\n",
        "\n",
        "    # Wir summieren Revenue und Sessions √ºber ALLE validen Shops\n",
        "    portfolio_agg = full_df.groupby(['Month', 'Channel']).agg({\n",
        "        'Revenue': 'sum',\n",
        "        'Sessions': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Berechnung des gewichteten Durchschnitts\n",
        "    portfolio_agg['RPS'] = portfolio_agg['Revenue'] / portfolio_agg['Sessions']\n",
        "\n",
        "    # Tabelle pivotieren\n",
        "    pivot = portfolio_agg.pivot(index='Month', columns='Channel', values='RPS').fillna(0)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üíé WEIGHTED REVENUE PER SESSION (Portfolio-Durchschnitt)\")\n",
        "    print(\"=\"*60)\n",
        "    print(pivot.round(2).to_markdown())\n",
        "\n",
        "    # --- PLOTTING ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Organic\n",
        "    sns.lineplot(data=portfolio_agg[portfolio_agg['Channel']=='Organic Search'], x='Month', y='RPS',\n",
        "                 color='gray', linewidth=2, label='Organic Search (Weighted Avg)')\n",
        "\n",
        "    # LLM\n",
        "    sns.lineplot(data=portfolio_agg[portfolio_agg['Channel']=='LLM / AI'], x='Month', y='RPS',\n",
        "                 color='#a64eff', linewidth=4, marker='o', markersize=8, label='LLM / AI (Weighted Avg)')\n",
        "\n",
        "    plt.title(f'Value per Session 2025: \"Dauerl√§ufer\" Portfolio (n={qualified_props_count})', fontsize=15)\n",
        "    plt.ylabel('Revenue / Session (√ò)', fontsize=12)\n",
        "    plt.xlabel('Monat')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.2)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- OPTIONAL: Einzelplots der qualifizierten Shops ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üîé EINZEL-ANALYSEN (Nur qualifizierte Shops)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Wir m√ºssen die Namen noch zuordnen, da wir oben DF gesammelt haben\n",
        "    # (Einfacher Fix: Wir plotten direkt hier nicht mehr, da der User nach \"hier nicht als median\" gefragt hat\n",
        "    # und der Fokus auf dem Aggregat liegt. Wenn Einzelplots gew√ºnscht, Code von vorher nutzen.)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_strict_rps_analysis()"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "tHAcMG9vPVd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üõçÔ∏è 5-Kanal Value Analyse (Active Shops Only)\n",
        "# ==============================================================================\n",
        "# Filter: Nur Shops mit Sales in JEDEM Monat 2025.\n",
        "# Metrik: Weighted Revenue per Session (Absoluter Wert).\n",
        "# Kan√§le: LLM, Ads, Social, Organic, Direct.\n",
        "# ==============================================================================\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# --- KONFIGURATION ---\n",
        "START_DATE = \"2025-01-01\"\n",
        "END_DATE   = \"2025-12-31\"\n",
        "\n",
        "LLM_REGEX = r\"^.*ai|.*\\.openai.*|.*copilot.*|.*chatgpt.*|.*gemini.*|.*gpt.*|.*neeva.*|.*writesonic.*|.*nimble.*|.*outrider.*|.*perplexity.*|.*google.*bard.*|.*bard.*google.*|.*bard.*|.*edgeservices.*|.*astastic.*|.*copy\\.ai.*|.*bnngpt.*|.*gemini.*google.*$\"\n",
        "SOCIAL_SOURCES = ['facebook', 'instagram', 'linkedin', 'twitter', 't.co', 'tiktok', 'pinterest', 'reddit', 'youtube']\n",
        "\n",
        "def get_creds_smart():\n",
        "    if 'creds' in globals() and globals()['creds'] and globals()['creds'].valid:\n",
        "        return globals()['creds']\n",
        "    try:\n",
        "        return authenticate_analytics()\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è Bitte Block 1 (Auth) zuerst ausf√ºhren.\")\n",
        "        return None\n",
        "\n",
        "def classify_5_channels(source, medium):\n",
        "    s = str(source).lower()\n",
        "    m = str(medium).lower()\n",
        "\n",
        "    # 1. LLM\n",
        "    if re.match(LLM_REGEX, s): return \"LLM / AI\"\n",
        "    # 2. Ads\n",
        "    if 'google' in s and ('cpc' in m or 'ppc' in m or 'paid' in m): return \"Google Ads\"\n",
        "    # 3. Direct\n",
        "    if '(direct)' in s: return \"Direct\"\n",
        "    # 4. Social\n",
        "    if 'social' in m or any(soc in s for soc in SOCIAL_SOURCES): return \"Social\"\n",
        "    # 5. Organic\n",
        "    if 'organic' in m: return \"Organic Search\"\n",
        "\n",
        "    return \"Other\"\n",
        "\n",
        "def get_ecommerce_data(service, property_id):\n",
        "    req = {\n",
        "        \"dateRanges\": [{\"startDate\": START_DATE, \"endDate\": END_DATE}],\n",
        "        \"dimensions\": [{\"name\": \"yearMonth\"}, {\"name\": \"sessionSource\"}, {\"name\": \"sessionMedium\"}],\n",
        "        \"metrics\": [{\"name\": \"sessions\"}, {\"name\": \"ecommercePurchases\"}, {\"name\": \"purchaseRevenue\"}]\n",
        "    }\n",
        "    try:\n",
        "        return service.properties().runReport(property=f\"properties/{property_id}\", body=req).execute()\n",
        "    except: return None\n",
        "\n",
        "def run_5_channel_shop_analysis():\n",
        "    creds = get_creds_smart()\n",
        "    if not creds: return\n",
        "\n",
        "    print(f\"\\nüöÄ Starte 5-Kanal Value Analyse (Nur aktive Shops)...\")\n",
        "\n",
        "    # Init\n",
        "    try:\n",
        "        admin_service = build('analyticsadmin', 'v1beta', credentials=creds)\n",
        "        data_service = build('analyticsdata', 'v1beta', credentials=creds)\n",
        "        acc_summary = admin_service.accountSummaries().list().execute()\n",
        "        all_props = [{'id': p['property'].split('/')[1], 'name': p.get('displayName')}\n",
        "                     for acc in acc_summary.get('accountSummaries', [])\n",
        "                     for p in acc.get('propertySummaries', [])]\n",
        "    except: return\n",
        "\n",
        "    valid_shops_data = []\n",
        "    qualified_props_count = 0\n",
        "\n",
        "    # Daten Loop\n",
        "    for prop in tqdm(all_props, desc=\"Filtere Shops\"):\n",
        "        res = get_ecommerce_data(data_service, prop['id'])\n",
        "        if not res or 'rows' not in res: continue\n",
        "\n",
        "        rows = []\n",
        "        for r in res['rows']:\n",
        "            rows.append({\n",
        "                'MonthStr': r['dimensionValues'][0]['value'],\n",
        "                'Source': r['dimensionValues'][1]['value'],\n",
        "                'Medium': r['dimensionValues'][2]['value'],\n",
        "                'Sessions': int(r['metricValues'][0]['value']),\n",
        "                'Purchases': int(r['metricValues'][1]['value']),\n",
        "                'Revenue': float(r['metricValues'][2]['value'])\n",
        "            })\n",
        "\n",
        "        df_raw = pd.DataFrame(rows)\n",
        "        if df_raw.empty: continue\n",
        "\n",
        "        # FILTER: Dauerl√§ufer-Check (Sales in jedem Monat)\n",
        "        monthly_check = df_raw.groupby('MonthStr')['Purchases'].sum()\n",
        "        if (monthly_check > 0).sum() < 12:\n",
        "            continue\n",
        "\n",
        "        qualified_props_count += 1\n",
        "\n",
        "        # Klassifizierung\n",
        "        df_raw['Channel'] = df_raw.apply(lambda x: classify_5_channels(x['Source'], x['Medium']), axis=1)\n",
        "        # Nur relevante Kan√§le\n",
        "        df_relevant = df_raw[df_raw['Channel'] != 'Other']\n",
        "\n",
        "        if df_relevant.empty: continue\n",
        "        valid_shops_data.append(df_relevant)\n",
        "\n",
        "    if not valid_shops_data:\n",
        "        print(\"‚ùå Keine aktiven Shops gefunden.\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚úÖ {qualified_props_count} Shops qualifiziert.\")\n",
        "\n",
        "    # --- AGGREGATION ---\n",
        "    full_df = pd.concat(valid_shops_data)\n",
        "    full_df['Date'] = pd.to_datetime(full_df['MonthStr'], format='%Y%m')\n",
        "\n",
        "    # Weighted Average Berechnung\n",
        "    portfolio_agg = full_df.groupby(['Date', 'Channel']).agg({\n",
        "        'Revenue': 'sum',\n",
        "        'Sessions': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    portfolio_agg['RPS'] = portfolio_agg['Revenue'] / portfolio_agg['Sessions']\n",
        "    portfolio_agg = portfolio_agg.sort_values('Date')\n",
        "\n",
        "    # --- TABELLE ---\n",
        "    portfolio_agg['Monat'] = portfolio_agg['Date'].dt.strftime('%Y-%m')\n",
        "    pivot = portfolio_agg.pivot(index='Monat', columns='Channel', values='RPS').fillna(0)\n",
        "\n",
        "    # Sortierung f√ºr Tabelle\n",
        "    desired_order = ['LLM / AI', 'Google Ads', 'Social', 'Organic Search', 'Direct']\n",
        "    cols = [c for c in desired_order if c in pivot.columns]\n",
        "    pivot = pivot[cols]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üí∞ VALUE PER SESSION (Absolute W√§hrung, Weighted Avg)\")\n",
        "    print(\"=\"*60)\n",
        "    print(pivot.round(2).to_markdown())\n",
        "\n",
        "    # --- PLOTTING ---\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    colors = {\n",
        "        'LLM / AI': '#a64eff',       # Violett\n",
        "        'Google Ads': '#DB4437',     # Rot\n",
        "        'Social': '#4285F4',         # Blau\n",
        "        'Organic Search': '#0F9D58', # Gr√ºn\n",
        "        'Direct': '#757575'          # Grau\n",
        "    }\n",
        "\n",
        "    for channel in cols:\n",
        "        subset = portfolio_agg[portfolio_agg['Channel'] == channel]\n",
        "        if subset.empty: continue\n",
        "\n",
        "        # LLM hervorheben\n",
        "        lw = 4 if channel == 'LLM / AI' else 2\n",
        "        style = '--' if channel == 'LLM / AI' else '-'\n",
        "        alpha = 1.0 if channel == 'LLM / AI' else 0.75\n",
        "\n",
        "        sns.lineplot(data=subset, x='Date', y='RPS',\n",
        "                     label=channel, color=colors.get(channel, 'black'),\n",
        "                     linewidth=lw, linestyle=style, marker='o', markersize=6, alpha=alpha)\n",
        "\n",
        "        # Label am Ende\n",
        "        last = subset.iloc[-1]\n",
        "        plt.text(last['Date'], last['RPS'], f\" {last['RPS']:.2f}\",\n",
        "                 color=colors.get(channel, 'black'), fontweight='bold', va='center')\n",
        "\n",
        "    plt.title(f'Value Comparison 2025: Welcher Kanal bringt den wertvollsten Traffic?\\n(Basis: {qualified_props_count} Dauerl√§ufer-Shops)', fontsize=16)\n",
        "    plt.ylabel('Revenue per Session (√ò)', fontsize=12)\n",
        "    plt.xlabel('')\n",
        "\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
        "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
        "    plt.gcf().autofmt_xdate()\n",
        "\n",
        "    plt.grid(True, alpha=0.2)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_5_channel_shop_analysis()"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "X4elwFKUP2GC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}